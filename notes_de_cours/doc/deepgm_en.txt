:ref:`Version française <deepgm_fr>`

.. _deepgm_en:

Probabilistic models for deep architectures
===========================================

Of particular interest are Boltzmann machine models, certain
variants of which are used in deep architectures such as
*Deep Belief Networks* and *Deep Boltzmann Machines*.
See section 5 of `Learning Deep Architectures for AI <http://www.iro.umontreal.ca/%7Elisa/publications2/index.php/publications/show/239>`_.

The Boltzmann distribution is generally defined on binary variables
:math:`x_i \in \{0,1\}`, with

.. math::

 P(x) = \frac{e^{x' W x + b'x}}{\sum_{\tilde x} \tilde{x}' W \tilde{x} + b'\tilde{x}}

where the denominator is simply a normalizing constant such that
:math:`\sum_x P(x)=1`, and the :math:`W_{ij}` indicates the nature
of the interaction (e.g. a positive value indicates that :math:`x_i`
et :math:`x_j` prefer to take the same value)  between pairs of
variables, and :math:`b_i` indicates the inclination of a given
:math:`x_i` to take a value of 1.


Readings on probabilistic graphical models
------------------------------------------

See

`Graphical models: probabilistic inference <http://www.cs.berkeley.edu/~jordan/papers/jordan-weiss.ps>`_.
M. I. Jordan and Y. Weiss. In M. Arbib (Ed.), The Handbook of Brain Theory and Neural Networks, 2nd edition.
Cambridge, MA: MIT Press, 2002.


Certain distributions can be written :math:`P(x)` for
a vector of variables :math:`x=(x_1,x_2,\ldots)` in the form

.. math::
 P(x) = \frac{1}{Z} \prod_c \psi_c(x_c)

where :math:`Z` is the normalizing constant (called the **partition function**),
and the product is over *cliques* (subsets :math:`x_c` of elements of the vector :math:`x`),
and the :math:`\psi_c(.)` are functions (one per clique) that indicate how
the variables in each clique interact.

A particular case where :math:`Z` may be simplified a bit (factorized
over cliques) is the case of *directed models* where variables
are structured as a directed acyclic graph, with a topological ordering
that associates a group of *parent variables* :math:`parents(x_i)` with
each variable :math:`x_i`:

.. math::
 P(x) = \prod_i P_i(x_i | parents(x_i))

where it can be seen that there is one clique for a variable and its parents,
i.e., :math:`P_i(x_i | parents(x_i)) = \psi_i(x_i, parents(x_i))/Z_i`.

In the general case (represented with an undirected graph), the
potential functions :math:`\psi_c` are directly parameterized,
often in the space of logarithms of :math:`\psi_c`,
leading to a formulation known as a **Markov random field**:

.. math::
 P(x) = \frac{1}{Z} e^{-\sum_c E_c(x_c)}

where:math:`E(x)=\sum_c E_c(x_c)` is called the **energy function**.
The energy function of a Boltzmann machine is a second degree polynomial
in :math:`x`. The most common parameterization of Markov random fields
has the following form, which is **log-linear**:

.. math::
 P(x) = \frac{1}{Z} e^{-\sum_c \theta_c f_c(x_c)}

where the only free parameters are the :math:`\theta_c`,
and where the complete log likelihood (when :math:`x`
is completely observed in each training example) is *log-linear*
in the parameters :math:`\theta`. One can easily
show that this function is  **convex** in :math:`\theta`.


Inference
---------

One of the most important obstacles in the practical application of
the majority of probabilistic models is the difficulty of **inference**:
given certain variables (a subset of :math:`x`),
predict the marginal distribution (separately for each) or joint
distribution of certain other variables.
Let :math:`x=(v,h)` with :math:`h` (*hidden*) being the variables
we would like to predict, and :math:`v` (*visible*)
being the observed subset.
One would like to calculate, or at least sample from,

.. math::
   P(h | v).

Inference is obviously useful if certain variables are missing, or
if, while using the model, we wish to predict a certain variable
(for example the class of an image) given some other variables
(for example, the image itself). Note that if the model
has hidden variables (variables that are *never* observed in the data)
we do not try to predict the values directly, but we will still implicitly
*marginalize* over these variables (sum over all configurations
of these variables).

Inference is also an essential component of learning, in order to
calculate gradients (as seen below in the case of Boltzmann
machines) or in the use of the Expectation-Maximization (EM) algorithm
which requires a marginalization over all hidden variables.

In general, exact inference has a computational cost exponential
in the size of the cliques of a graph (in fact, the unobserved
part of the graph) because we must consider all possible combinations
of values of the variables in each clique. See section 3.4
of `Graphical models: probabilistic inference <http://www.cs.berkeley.edu/~jordan/papers/jordan-weiss.ps>`_
for a survey of exact inference methods.

A simplifed form of inference consists of calculating not the entire
distribution, but only the mode (the most likely configuration of
values) of the distribution:

.. math::
   h^* = {\rm argmax}_{h} P(h | v)

This is known as **MAP = Maximum A Posteriori** inference.

Approximate inference
---------------------

The two principal families of methods for approximate inference in
probabilistic models are Markov chain Monte Carlo (MCMC) methods and
variational inference.

The principle behind variational inference is the following. We will define a
simpler model than the target model (the one that interests us), in which
inference is easy, with a similar set of variables (though generally with more
simple dependencies between variables than those contained in the target
model). We then optimize the parameters of the simpler model so as to
approximate the target model as closely as possible. Finally, we do inference
using the simpler model.  See section 4.2 of `Graphical models: probabilistic
inference <http://www.cs.berkeley.edu/~jordan/papers/jordan-weiss.ps>`_ for
more details and a survey.

Inference with MCMC
-------------------

In general :math:`P(h | v)` can be exponentially expensive to represent (in
terms of the number of hidden variables, because we must consider all possible
configurations of :math:`h`).  The principle behind Monte Carlo inference is
that we can approximate the distribution :math:`P(h | v)` using samples from
this distribution.  Indeed, in practice we only need an expectation (for
example, the expectation of the gradient) under this conditional distribution.
We can thus approximate the desired expectation with an average of these
samples.

See the page `site du zéro sur Monte-Carlo
<http://www.siteduzero.com/tutoriel-3-133680-toute-la-puissance-de-monte-carlo.html>`_
(in French) for a gentle introduction.

Unfortunately, for most probabilistic models, even sampling from :math:`P(h |
v)` exactly is not feasible (taking time exponential in the dimension of de
:math:`h`). Therefore the most general approach is based on an *approximation*
of Monte-Carlo sampling called Markov chain Monte Carlo (MCMC).

A (first order) Markov chain is a sequence of random variables
:math:`Z_1,Z_2,\ldots`, where :math:`Z_k` is independent of
:math:`Z_{k-2}, Z_{k-3}, \ldots` given :math:`Z_{k-1}`:

.. math::
  P(Z_k | Z_{k-1}, Z_{k-2}, Z_{k-3}, \ldots) = P(Z_k | Z_{k-1})

  P(Z_1 \ldots Z_n) = P(Z_1) \prod_{k=2}^n P(Z_k|Z_{k-1})

The goal of MCMC is to construct a Markov chain whose asymptotic
marginal distribution, i.e. the distribution of :math:`Z_n` as
:math:`n \rightarrow \infty`, converges towards a given target
distribution, such as  :math:`P(h | v)` or :math:`P(x)`.

Gibbs sampling
--------------

Numerous MCMC-based sampling methods exist. The one most commonly
used for deep architectures is **Gibbs sampling**. It is simple
and has a certain plausible
analogy with the functioning of the brain, where each neuron
decides to send signals with a certain probability as a function
of the signals it receives from other neurons.

Let us suppose that we wish to sample from the distribution :math:`P(x)`
where :math:`x` is a set of variables :math:`x_i` (we could optionally
have a set of variables upon which we have conditioned, but this would
not change the procedure, so we ignore them in the following description).

Let :math:`x_{-i}=(x_1,x_2,\ldots,x_{i-1},x_{i+1},\ldots,x_n)`, i.e.
all variables in :math:`x` excluding :math:`x_i`. Gibbs sampling is
performed using the following algorithm:

* Choose an initial value of :math:`x` in an arbitrary manner (random or not)
* For each step of the Markov chain:

  * Iterate over each :math:`x_k` in :math:`x`

    * Draw :math:`x_k` from the conditional distribution :math:`P(x_k | x_{-k})`

In some cases one can group variables in :math:`x` into *blocks* or groups of
variables such that drawing samples for an entire group, given the others, is
easy. In this case it is advantageous to interpret the algorithm above with
:math:`x_i` as the :math:`i^{\mathrm{th}}` group rather than the
:math:`i^{\mathrm{th}}` variable.  This is known as *block Gibbs sampling*.

The gradient in a log-linear Markov random field
------------------------------------------------

See
`Learning Deep Architectures for AI
<http://www.iro.umontreal.ca/%7Elisa/publications2/index.php/publications/show/239>`_
for detailed derivations.

Log-linear Markov random fields are undirected probabilistic models where
the energy function is *linear in the parameters* :math:`\theta` of the model:

.. math::
  P(x) \propto e^{-\sum_i \theta_i f_i(x)}

where :math:`f_i(.)` are known as *sufficient statistics*
of the model, because the expectations :math:`E[f_i(x)]` are
sufficient for characterizing the distribution and estimating parameters.

Note that :math:`e^{\theta_i f_i(x)} = \psi_i(x)` is associated with
each clique in the model (in general, only a sub-vector of :math:`x`
influences each :math:`f_i(x)`).

Getting back to sufficient statistics, one can show that the
gradient of the log likelihood is as follows:

.. math::
  \frac{- \partial \log P(x)}{\partial \theta_i} = f_i(x) - \sum_x P(x) f_i(x)

and the average gradient over training examples :math:`x_t` is thus

.. math::
  \frac{1}{T} \sum_t \frac{-\partial log P(x_t)}{\partial \theta_i} =
            \frac{1}{T}\sum_t f_i(x_t) - \sum_x P(x) f_i(x)

Thus, it is clear that the gradient vanishes when *the average
of the sufficient statistics under the training distribution
equals their expectation under the model distribution :math:`P`*.

Unfortunately, calculating this gradient is difficult. We do not want
to sum over all possible :math:`x`, but fortunately one can obtain
a Monte-Carlo approximation  by one or more samples from :math:`P(x)`,
which gives us a noisy estimate of the gradient. In general, however,
even to obtain an unbiased sample from :math:`P(x)` is exponentially
costly, and thus one must use an MCMC method.

We refer to the terms of the gradient due to the numerator of
the probability density (:math:`-f_i(x)`) as the **'positive phase'**
gradient, and the terms of the gradient corresponding to the partition
function (denominator of the probability density) as the
**'negative phase'** gradient.

Marginalization over hidden variables
-------------------------------------

When a model contains hidden variables, the gradient becomes
a bit more complicated since one must marginalize over the
hidden variables. Let :math:`x=(v,h)`, with :math:`v` being the visible
part and :math:`h` being the hidden part, with statistics
from functions of the two, :math:`f_i(v,h)`. The average gradient
of the negative log likelihood of the observed data is thus

.. math::
  \frac{1}{T} \sum_t \frac{-\partial \log P(v_t)}{\partial \theta_i} =
            \frac{1}{T}\sum_t \sum_h P(h|v_t) f_i(v_t,h) - \sum_{h,v} P(v,h) f_i(v,h).


In the general case, it will be necessary to resort to MCMC
not only for the negative phase gradient but also for the positive
phase gradient, i.e. to sample :math:`P(v|h_t)`.

.. _dbm_en:

The Boltzmann Machine
=====================

A Boltzmann machine is an undirected probabilistic model, a particular
form of log-linear *Markov random field*, containing both visible and hidden
variables, where the *energy function* is a *second degree polynomial* of
the variables :math:`x`:

.. math::
   E(x) = -d'x - x'Ax

The classic Boltzmann machine has binary variables and
inference is conducted via Gibbs sampling, which requires samples
from :math:`P(x_i | x_{-i})`. It can be easily shown that

.. math::
   P(x_i=1 | x_{-i}) = {\rm sigmoid}(d_i + \omega_i x_{-i})

where :math:`\omega_i` is the :math:`i^{th}` row of :math:`A` excluding
the :math:`i^{th}` element (the diagonal of :math:`A`
is 0 in this model). Thus, we see the link with networks of neurons.

.. _rbm:

Restricted Boltzmann Machines
-----------------------------

A *Restricted Boltzmann Machine*, or RBM, is a Boltzmann machine without
*lateral connections* between the visible units :math:`v_i` or between
the hidden units :math:`h_i`. The energy function thus becomes

.. math::
   E(v,h) = -b'h - c'v - v'W h.

where the matrix :math:`A` is entirely 0 except in the submatrix :math:`W`.
The advantage of this connectivity restriction is that inferring
:math:`P(h|v)` (and also :math:`P(v|h)`) becomes very easy, can be performed
analytically, and the distribution *factorizes*:

.. math::
   P(h|v) = \prod_i P(h_i|v)

and

.. math::
   P(v|h) = \prod_i P(v_i|h)

In the case where the variables ("units") are binary, we obtain once again
have a sigmoid activation probability:

.. math::
   P(h_j=1 | v) = {\rm sigmoid}(b_j + \sum_i W_{ij} v_i)

   P(v_i=1 | h) = {\rm sigmoid}(c_i + \sum_j W_{ij} h_j)

Another advantage of the RBM is that the distribution :math:`P(v)` can
be calculated analytically up to a constant (the unknown constant
being the partition function). This permits us to define a generalization
of the notion of an energy function in the case when we wish to marginalize
over the hidden variables: the **free energy** (inspired by notions from
physics)

.. math::
   P(v) = \frac{e^{-FE(v)}}{Z} = \sum_h P(v,h) = \frac{\sum_h e^{-E(v,h)}}{Z}

   FE(v) = -\log \sum_h e^{-E(v,h)}

and in the case of RBMs, we have

.. math::
   FE(v) = -b'v - \sum_i \log \sum_{h_i} e^{h_i (c_i + v' W_{.i})}

where the sum over :math:`h_i` is a sum over values that the hidden variables
can take, which in the case of binary units yields

.. math::
   FE(v) = -b'v - \sum_i \log (1 + e^{c_i + v' W_{.i}})

   FE(v) = -b'v - \sum_i {\rm softplus}(c_i + v' W_{.i})


Gibbs sampling in RBMs
----------------------

Although sampling from :math:`P(h|v)` is easy and immediate in an RBM,
drawing samples from :math:`P(v)` or from :math:`P(v,h)` cannot be done
exactly and is thus generally accomplished with MCMC, most commonly with
*block Gibbs sampling*, where we take advantage of the fact that
sampling from :math:`P(h|v)` and :math:`P(v|h)` is easy:

.. math::
   v^{(1)} \sim {\rm exemple\;\; d'apprentissage}

   h^{(1)} \sim P(h | v^{(1)})

   v^{(2)} \sim P(v | h^{(1)})

   h^{(2)} \sim P(h | v^{(2)})

   v^{(3)} \sim P(v | h^{(2)})

   \ldots

In order to visualize the generated data at step :math:`k`, it is better
to use expectations (i.e. :math:`E[v^{(k)}_i|h^{(k-1)}]=P(v^{(k)}_i=1|h^{(k-1)})`)
which are less noisy than the samples :math:`v^{(k)}` themselves.

.. _trainrbm:

Training RBMs
=============

The exact gradient of the parameters of an RBM (for an example :math:`v`) is

.. math::
   \frac{\partial \log P(v)}{\partial W} = v' E[h | v] - E[v' h]

   \frac{\partial \log P(v)}{\partial b} = E[h | v] - E[h]

   \frac{\partial \log P(v)}{\partial c} = v - E[v]

where the expectations are under the distribution of the RBM. The conditional
expectations can be calculated analytically (since :math:`E[h_i | v]=P(h_i=1|v)=` the output of a hidden unit, for binary :math:`h_i`)
but the unconditional expectations must be approximated using MCMC.

Contrastive Divergence
----------------------

The first and simplest approximation of :math:`E[v' h]`, i.e., for
obtaining 'negative examples' (for the 'negative phase' gradient),
consists of running a short Gibbs chain (of :math:`k` steps) *beginning at
a training example*.  This algorithm is known as CD-k
(*Contrastive Divergence with k steps*). See algorithm 1
in `Learning Deep Architectures for AI <http://www.iro.umontreal.ca/%7Elisa/publications2/index.php/publications/show/239>`_:

.. math::
  W \leftarrow W + \epsilon( v^{(1)} \hat{h}^{(1)'} - v^{(2)} \hat{h}^{(2)'} )

  b \leftarrow b + \epsilon( \hat{h}^{(1)} - \hat{h}^{(2)} )

  c \leftarrow c + \epsilon( v^{(1)} - v^{(2)} )

where :math:`\epsilon` is the gradient step size, and we refer to the
notation for Gibbs sampling from RBMs above, with
:math:`\hat{h}^{(1)}` denotes the vector of probabilities :math:`P(h^{(1)}_i=1|v_1)`
and in the same fashion :math:`\hat{h}^{(2)}_i=P(h^{(2)}_i=1|v^{(2)})`.

What is surprising is that even with :math:`k=1`, we obtain
RBMs that work well in the sense that they extract good features
from the data (which we can verify visually byt looking at the filters,
the stochastic reconstructions after one step of Gibbs, or quantitatively
by initializing each layer of a deep network with :math:`W` and
:math:`b` obtained by pretraining an RBM at each layer).

It can be shown that CD-1 is very close to the training procedure of an
autoencoder by minimizing reconstruction error, and one can see that
the reconstruction error diminishes in a mostly monotonic fashion during CD-1
training.

It can also be shown that CD-k tends to the true gradient (in expected value)
when k becomes large, but at the same time increases computation time by
a factor of k.

Persistent Contrastive Divergence
---------------------------------

In order to obtain a less biased estimator of the true gradient without
significantly increasing the necessary computation time, we can use the
*Persistent Contrastive Divergence* (PCD) algorithm. Rather than restarting
a Gibbs chain after each presentation of a training example :math:`v`,
PCD keeps a chain running in order to obtain negative examples. This chain
is a bit peculiar because its transition probabilities change (slowly) as
we update the parameters of the RBM. Let :math:`{v^-, h^-}` be the state
of our negative phase chain. The learning algorithm is then


.. math::
  \hat{h}_i = P(h_i=1 | v)

  \forall i, \hat{v}^-_i = P(v_i=1 | h^-)

  v^- \sim \hat{v}^-

  \forall i, \widehat{h_i}^- = P(h_i=1 | v^-)

  h^- \sim \hat{h}^-

  W \leftarrow W + \epsilon( v \hat{h}' - v^- \hat{h}^{-'} )

  b \leftarrow b + \epsilon( \hat{h} - \hat{h}^- )

  c \leftarrow c + \epsilon( v - \hat{v}^- )


Experimentally we find that PCD is better in terms of generating examples
that resemble the training data (and in terms of the likelihood
:math:`\log P(v)`) than CD-k, and is less sensitive to the initialization
of the Gibbs chain.

.. _dbn:

Stacked RBMs and DBNs
=====================

RBMs can be used, like autoencoders, to pretrain a deep neural network in an
unsupervised manner, and finish training in the usual supervised fashion.
One stacks RBMs with the hidden layer of one (given its input) i.e., les
:math:`P(h|v)` or :math:`h \sim P(h|v)`, becomes the data for the next
layer.

The pseudocode for *greedy* layer-by-layer training of a stack of RBMs
is presented in section 6.1 (algorithm 2) of `Learning Deep Architectures
for AI <http://www.iro.umontreal.ca/%7Elisa/publications2/index.php/publications/show/239>`_. To train the :math:`k^{\mathrm{th}}` RBM, we propagate forward
samples :math:`h \sim P(h|v)` or the posteriors :math:`P(h|v)` through
the :math:`k-1` previously trained RBMs and use them as data for training
the :math:`k^{\mathrm{th}}` RBM. They are trained one at a time: once
we stop training the :math:`k^{\mathrm{th}}`, we move on to the
:math:`{k+1}^{\mathrm{th}}`.

An RBM has the same parameterization as a layer in a classic neural network
(with logistic sigmoid hidden units), with the difference that we use
only the weights :math:`W` and the biases :math:`b` of the hidden units
(since we only need :math:`P(h|v)` and not :math:`P(v|h)`).

Deep Belief Networks
--------------------

We can also consider a stacking of RBMs in a generative manner, and we call
these models Deep Belief Networks:

.. math::

  P(x,h^1,\ldots,h^{\ell}) = \left( \prod_{k=0}^{\ell-2} P(h^k | h^{k+1}) \right) P(h^{\ell-1}, h^{\ell})

where we denote :math:`x=h^0` and :math:`h^k` as the random variable (vector)
associated with layer :math:`k`. The last two layers have a joint distribution
given by an RBM (the last of the stack). The RBMs below serve
only to define the conditional probabilities :math:`P(h^k | h^{k+1})`
of the DBN, where :math:`h^k` play the role of visible units and
:math:`h^{k+1}`similarly play the role of hidden units in RBM k+1.


Sampling from a DBN is thus performed as follows:

 * Sample a :math:`h^{\ell-1}` from the top RBM (number :math:`\ell`), for
   example by running a Gibbs chain
 * For k from :math:`\ell-1` to 1
    * sample the visible units (:math:`h^k`) given the hidden units
      (:math:`h^{k+1}`) in RBM k
 * Return :math:`h^k`, the last sample obtained, which is the result
   of generating from the DBN

Unfolding an RBM and RBM - DBN equivalence
------------------------------------------

It can be shown (see section 8.1 of
`Learning Deep Architectures for AI <http://www.iro.umontreal.ca/%7Elisa/publications2/index.php/publications/show/239>`_.)
that an RBM corresponds to DBN with a particular architecture, where the
weights are shared between all the layers:
level 1 of the DBN uses the weights :math:`W` of the RBM,
level 2 uses the weights :math:`W'`, level 3 uses :math:`W`, etc. alternating
between :math:`W` and :math:`W'`. The last pair of layers of the DBN is an RBM with weights :math:`W` or :math`W'` depending on whether the number of layers
is odd or even.
Note that in this equivalence, the DBN has layer sizes that alternate
(number of visible units of the RBM, number of hidden units of the RBM,
number of visibles, etc.)

In fact we can continue the unfolding of an RBM infinitely and obtain
an infinite directed network with shared weights, equivalently.
See figure 13 in the same section, 8.1.

It can be seen that this infinite network corresponds exactly to an infinite
Gibbs chain that leads to (finishes on) the visible layer of the original
RBM, i.e. that generates the same examples.  The even layers correspond to
sampling :math:`P(v|h)` (of the original RBM) and the odd layers to
sampling :math:`P(h|v)`.

Finally, it can be shown that if we take an RBM and unfold it one time
(mirrored), the continued training of the new RBM on top (initialized
with :math:`W'`) maximizes a lower bound on the log likelihood of the
corresponding DBN. In passing from an RBM to a DBN, we replace the marginal
distribution :math:`P(h)` of the RBM (which is encoded implicitly in
the parameters of the RBM) with the distribution generated by the part of
the DBN above this RBM (the DBN consists of all layers above :math:`h`),
since this :math:`h` corresponds to visible units of this DBN. The proof
is simple and instructive, and uses the letter Q for the probabilities
according to the RBM (at the bottom) and the letter P for the probabilities
according to the DBN obtained by modeling :math:`h` differently (i.e.
by replacing :math:`Q(h)` by :math:`P(h)`). We also remark that
:math:`P(x|h) = Q(x|h)`, but this is not true for :math:`P(h|x)` and
:math:`Q(h|x)`.

.. math::

  \log P(x) = \left(\sum_{h} Q(h|x)\right) \log P(x) = \sum_{h} Q(h|x) \log \frac{P(x,h)}{P(h|x)}

  \log P(x) = \sum_{h} Q(h|x) \log \frac{P(x,h)}{P(h|x)} \frac{Q(h|x)}{Q(h|x)}

  \log P(x) = H_{Q(h|x)} + \sum_{h} Q(h|x) \log P(x, h) + \sum_{h} Q(h|x) \log \frac{Q(h|x)}{P(h|x)}

  \log P(x) = KL(Q(h|x)||P(h|x)) + H_{Q(h|x)} + \sum_{h} Q(h|x) \left(\log P(h) + \log P(x|h) \right)

  \log P(x) \geq \sum_{h} Q(h|x) \left(\log P(h) + \log P(x|h) \right)

This shows that one can actually increase the lower bound (last line) by
doing maximum likelihood training of :math:`P(h)` using as training data the
:math:`h` drawn from :math:`Q(h|x)`, where :math:`x` is drawn from the
training distribution of the bottom RBM. Since we have decoupled the weights
below from those above, we don't touch the bottom RBM (:math:`P(x|h)` and
:math:`Q(h|x)`), and only modify :math:`P(h)`.

Approximate inference in DBNs
-----------------------------

Contrary to the RBM, inference in DBNs (inferring the states of the hidden
units given the visible units) is very difficult. Given that we
initialize DBNs as a stack of RBMs, in practice the following approximation
is used: sample the :math:`h^k` given the :math:`h^{k-1}` using the new
weights of level :math:`k`.  This would be exact inference if this was
still an isolated RBM, but it is no longer exact with the DBN.

We saw that this is an approximation in the previous section because the
marginal :math:`P(h)` (of the DBN) differs from the marginal :math:`Q(h)`
(of the bottom RBM), after modifying the upper weights so that they are
no longer the tranpose of the bottom weights, and thus :math:`P(h|x)`
differs from :math:`Q(h|x)`.

Deep Boltzmann Machines
-----------------------

Finally, we can also use a stack of RBMs for initializing a deep Boltzmann
machine (Salakhutdinov and Hinton, AISTATS 2009). This is a Boltzmann machine
organized in layers, where each layer is connected the layer below and
the layer above, and there are no within-layer connections.

Note that the weights are somehow two times too big when doing the
initialization described above, since now each unit receives input from
the layer above and the layer below, whereas in the original RBM it was
either one or the other. Salakhutdinov proposes thus dividing the weights
by two when we make the transition from stacking RBMs to deep Boltzmann
machines.

It is also interesting to note that according to Salakhutdinov, it is
crucial to initialize deep Boltzmann machines as a stack of RBMs, rather
than with random weights. This suggests that the difficulty of training
deep deterministic MLP networks is not unique to MLPs, and that a similar
difficulty is found in deep Boltzmann machines. In both cases, the
initialization of each layer according to a local training procedure
seems to help a great deal. Salakhutdinov obtains better results with
his deep Boltzmann machine than with an equivalent-sized DBN, although
training the deep Boltzmann machine takes longer.
